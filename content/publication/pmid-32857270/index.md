---
title: "[Artificial intelligence in ophthalmology : Guidelines for physicians for the critical evaluation of studies]"
date: 2020-10-01
publishDate: 2022-02-12T23:10:12.727307Z
authors: ["M. Pfau", "G. Walther", "L. von der Emde", "P. Berens", "L. Faes", "M. Fleckenstein", "T. F. C. Heeren", "K. Kortüm", "S. H. Künzel", "P. L. Müller", "P. M. Maloca", "S. M. Waldstein", "M. W. M. Wintergerst", "S. Schmitz-Valckenberg", "R. P. Finger", "F. G. Holz"]
publication_types: ["2"]
abstract: "Empirical models have been an integral part of everyday clinical practice in ophthalmology since the introduction of the Sanders-Retzlaff-Kraff (SRK) formula. Recent developments in the field of statistical learning (artificial intelligence, AI) now enable an empirical approach to a wide range of ophthalmological questions with an unprecedented precision. Which criteria must be considered for the evaluation of AI-related studies in ophthalmology? Exemplary prediction of visual acuity (continuous outcome) and classification of healthy and diseased eyes (discrete outcome) using retrospectively compiled optical coherence tomography data (50 eyes of 50 patients, 50 healthy eyes of 50 subjects). The data were analyzed with nested cross-validation (for learning algorithm selection and hyperparameter optimization). Based on nested cross-validation for training, visual acuity could be predicted in the separate test data-set with a mean absolute error (MAE, 95% confidence interval, CI of 0.142 LogMAR [0.077; 0.207]). Healthy versus diseased eyes could be classified in the test data-set with an agreement of 0.92 (Cohen's kappa). The exemplary incorrect learning algorithm and variable selection resulted in an MAE for visual acuity prediction of 0.229 LogMAR [0.150; 0.309] for the test data-set. The drastic overfitting became obvious on comparison of the MAE with the null model MAE (0.235 LogMAR [0.148; 0.322]). Selection of an unsuitable measure of the goodness-of-fit, inadequate validation, or withholding of a null or reference model can obscure the actual goodness-of-fit of AI models. The illustrated pitfalls can help clinicians to identify such shortcomings. HINTERGRUND: Empirische Modelle sind seit Einführung der SRK(Sanders-Retzlaff-Kraff)-Formel im klinischen Alltag der Augenheilkunde etabliert. Rezente Entwicklungen im Bereich des statistischen Lernens („künstliche Intelligenz“ [KI]) ermöglichen jetzt ein empirisches Vorgehen für vielfältigste ophthalmologische Fragestellungen bei bislang unerreichter Präzision. Welche Kriterien müssen für die Bewertung von Arbeiten zum Thema KI in der Augenheilkunde berücksichtigt werden? Es erfolgen die beispielhafte Vorhersage des Visus (stetige Zielgröße) und Klassifikation von gesunden und kranken Augen (diskrete Zielgröße) anhand von retrospektiven optischen Kohärenztomographiebilddaten (50 Augen von 50 Patienten, 50 gesunde Augen von 50 Probanden). Die Daten wurden mit verschachtelter Kreuzvalidierung (zur Lernalgorithmusauswahl und Hyperparameteroptimierung) analysiert. Durch verschachtelte Kreuzvalidierung ließ sich der Visus im separaten Testdatensatz mit einem mittleren absoluten Fehler (MAE, [95 %-CI, Konfidenzintervall]) von 0,142 LogMAR [0,077; 0,207] vorhersagen. Kranke und gesunde Augen ließen sich im Testdatensatz mit einer Konkordanz von (Kappa nach Cohen) 0,92 klassifizieren. Die beispielhafte inkorrekte Lernalgorithmus- und Variablenauswahl resultierte in einem MAE von 0,229 LogMAR [0,150; 0,309] für den Testdatensatz. Erst durch Vergleich mit dem MAE des Nullmodells (0,235 LogMAR [0,148; 0,322]) wurde die Überanpassung offensichtlich. Die Auswahl einer ungeeigneten Kennzahl für die Anpassungsgüte, inadäquate Validierung oder Unterschlagen eines Null- oder Referenzmodells kann die tatsächliche Anpassungsgüte von KI-Modellen verschleiern. Die illustrierten Fallstricke können Klinikern und Forschern helfen, solche Unzulänglichkeiten zu erkennen."
featured: false
publication: "*Ophthalmologe*"
---

