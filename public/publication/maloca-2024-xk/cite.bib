@article{Maloca2024-xk,
 abstract = {Supervised deep learning (DL) algorithms are highly dependent on
training data for which human graders are assigned, for example,
for optical coherence tomography (OCT) image annotation. Despite
the tremendous success of DL, due to human judgment, these
ground truth labels can be inaccurate and/or ambiguous and cause
a human selection bias. We therefore investigated the impact of
the size of the ground truth and variable numbers of graders on
the predictive performance of the same DL architecture and
repeated each experiment three times. The largest training
dataset delivered a prediction performance close to that of
human experts. All DL systems utilized were highly consistent.
Nevertheless, the DL under-performers could not achieve any
further autonomous improvement even after repeated training.
Furthermore, a quantifiable linear relationship between ground
truth ambiguity and the beneficial effect of having a larger
amount of ground truth data was detected and marked as the
more-ground-truth effect.},
 author = {Maloca, Peter M and Pfau, Maximilian and Janeschitz-Kriegl,
Lucas and Reich, Michael and Goerdt, Lukas and Holz, Frank G and
MÃ¼ller, Philipp L and Valmaggia, Philippe and Fasler, Katrin
and Keane, Pearse A and Zarranz-Ventura, Javier and Zweifel,
Sandrine and Wiesendanger, Jonas and Kaiser, Pascal and Enz, Tim
J and Rothenbuehler, Simon P and Hasler, Pascal W and Juedes,
Marlene and Freichel, Christian and Egan, Catherine and Tufail,
Adnan and Scholl, Hendrik P N and Denk, Nora},
 copyright = {http://onlinelibrary.wiley.com/termsAndConditions#vor},
 journal = {J. Biophotonics},
 keywords = {explainable AI; machine learning; optical coherence tomography;
retina},
 language = {en},
 month = {February},
 number = {2},
 pages = {e202300274},
 publisher = {Wiley},
 title = {Human selection bias drives the linear nature of the more ground
truth effect in explainable deep learning optical coherence
tomography image segmentation},
 volume = {17},
 year = {2024}
}

